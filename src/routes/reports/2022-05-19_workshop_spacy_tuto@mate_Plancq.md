---
title: 'Tuto@mate - de lâ€™utilisation de spaCy, comment parser un texte avec une bibliothÃ¨que Python'
date: '2022-05-19'
speaker: 'ClÃ©ment Plancq'
category: 'workshop'
keywords: ''
---

# Tuto@mate - de lâ€™utilisation de spaCy, comment parser un texte avec une bibliothÃ¨que Python

## Ressources
 - [Notebook du tuto@mate](https://github.com/clement-plancq/tuto-mate) pour tester le code sans installer Python et spaCy (cliquer sur launch binder dans le README.md)
 - [Cours par Ines Montani](https://course.spacy.io/fr/)

## Introduction

SpaCy est un librairie Python pour le TAL, developpÃ©e par Matthew Honnibal et Ines Montani. C'est une bibliothÃ¨que Python pour le traitement des TAL sous licence libre (MIT 2.0), bien que distribuer par une sociÃ©tÃ© privÃ©e. En revanche les modÃ¨les sont distribuÃ©s sous divers licences ouvertes (liÃ©es aux documents qui ont permis leur Ã©laboration).

SpaCy est destinÃ© Ã  Ãªter utilisÃ© en productionÂ : donc traitement rapide, stable, qualitÃ© du code (test, documentation, etc.). Mais pas de chois dans le mÃ©thode ou l'algorithme utilisÃ©. On sait ce qui est utilisÃ©, c'est documentÃ©, mais on ne peut pas modifier ces paramÃ¨tres.

Peut faireÂ : 
- tokenisation
- Ã©tiquetage POS (verbe, nom, etc.)
- analyse syntaxique
- detection d'entitÃ©s nommÃ©es
- lemmatisation
- catÃ©gorisation de texte
- word embedding

SpaCy utilise des modÃ¨les statistique (mÃ©thode neuronales).

IntÃ©rÃªtsÂ : c'est dy Python (wrapper R Ã©galement), simple Ã  prendre en main, trÃ¨s bien documentÃ© (doc, tuto, etc.), grosse communautÃ© sur github, fournit les mÃ©thodes et les moyens d'adapter le traitement et ou le modÃ¨le Ã  des besoins particuliers ! Mais ce n'est pas forcÃ©ment l'outils qui sera le meilleurs pour le franÃ§ais dans toutes les tÃ¢ches de TAL.

Il existe d'autre frameworks pour le TAL
- NLKTÂ : python, orientÃ© pÃ©dagogie, chois des mÃ©thodes et algos Ã  utiliser
- CoreNLPÂ : java, framework de Stanford, orientÃ© recherche, chaÃ®ne de TAL la plus complÃ¨te pour l'anglais
- StanzaÂ : python, framework de Stanford Ã©galement, modÃ¨le neuronaux entraÃ®nÃ©s sur les donnÃ©es d'Universal Dependancies (il y a un module spacy-stanza pour utiliser les modÃ¨les de Stanza
- FlairÂ : python, framewok de Zalando (site de e-commerce), peut Ãªtre le plus performant sur la reconnaissance d'entitÃ©s nommÃ©es

Les modÃ¨les de spaCy
SpaCy utilise des modÃ¨les statistiques qui permettent de prÃ©dire des annotations linguistiques (identifier un verbe, un nom, le sentiment d'une phrase).
21 langues prise en compte. La qualitÃ© des rÃ©sultats dÃ©pend beaucoup du corpus sur lequel s'est entraÃ®nÃ© le modÃ¨le. 
Pour le franÃ§ais, il y a 3 modÃ¨les + 1. Il sont tous issus du corpus [Sequoia](http://www.linguist.univ-paris-diderot.fr/~mcandito/Publications/candito-seddah-taln2012.pdf) (wikipedia + presse (Est-rÃ©publicain) + Agence euro du mÃ©dicament, Europarl) et WikiNer, sauf le modÃ¨le trf, qui est issu de camembert-base (modÃ¨le Bert), distribuÃ© par Hugginf Face, entraÃ®nÃ© sur Oscar.

Le choix du modÃ¨le est vraiment primordial pour les traitements.

## Ã‰tape de la chaÃ®ne de traitement
### Tokenisation
OpÃ©ration qui consiste Ã  dÃ©couper un texte ou chaÃ®ne de caractÃ¨res en token. En linguistique il n'y a pas de dÃ©finition prÃ©cise de Â« mot Â», on parle alors de token. Les signes de ponctuation sont tokÃ©nisÃ©s.

L'intÃ©rÃªt de spaCy, c'est que sa tokÃ©nisation n'est pas destructive. On peut donc, Ã  partir de la tokenisation, reformer le texte (conservation des espaces par exemple).

Il est possible de modifier la tokenisation par dÃ©faut. 

Concernant la tokÃ©nisation, SpaCy permet aussi de rÃ©cupÃ©rer des groupes nominaux. Il est Ã©galement possible de rÃ©cupÃ©rer les phrases d'un texte.

### Ã‰tiquetage (tagging)
Ã‰tape qui permet, pour chaque token de dÃ©terminer s'il s'agit d'un verbe, du adjectif, d'un nom, etc.

```python
doc = nlp("Tous mes beaux chÃ¢teaux d'Ã‰quateur s'Ã©croulent.")
for tok in doc:
    print(tok, tok.pos_)
```
    > Tous ADJ
    mes DET
    beaux ADJ
    chÃ¢teaux NOUN
    d' ADP
    Ã‰quateur PROPN
    s' PRON
    Ã©croulent VERB
    . PUNCT

Les annotation portant sur les token sont accessibles via les attributs des objets de type token
- pos_ contient l'Ã©tiquette de partie du discours de universal dependancies
- tag_ contient l'Ã©tiquette du corpus original, parfois plus dÃ©taillÃ©e
- lemma_ pour le lemme
- morph pour l'analyse morphologique
```python
for token in doc:
    print(token, token.lemma_, token.pos_, token.morph)
```
    > Tous tout ADJ Gender=Masc|Number=Plur
    mes mon DET Number=Plur|Poss=Yes
    beaux beal ADJ Gender=Masc|Number=Plur
    chÃ¢teaux chÃ¢teau NOUN Gender=Masc|Number=Plur
    d' de ADP
    Ã‰quateur Ã‰quateur PROPN
    s' se PRON Person=3|Reflex=Yes
    Ã©croulent Ã©crouler VERB Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin
    . . PUNCT

### DÃ©tection d'entitÃ©s nommÃ©es (ner)
Pour repÃ©rer les noms de personnes, de lieux, d'organisation, miscelannÃ©e et parfois mÃªme les dates.

Il y a Ã©galement dans spaCy un visualiseur, qui permet d'avoir une reprÃ©sentation visuel du traitement du texte en html.

Il est possible d'adapter la reconnaissance des entitÃ©s nommÃ©es, voire mÃªme d'entrainer d'autres modÃ¨les.

### Analyse syntaxique (parsing)
C'est une analyse en dÃ©pendance avec spaCy. Dans ce type d'analyse, chaque mot d'une phrase Ã  un gouverneur unique (head), la relation de dÃ©pendance entre le mot et son gouverneur est typÃ©e (nsubj, obj, etc.) Pour la tÃªte de la phrase on utilise la relation ROOT.

La structure produite par l'analyse syntaxique est un arbre, un graphe acyclique et connexe.
Les tokens sont les nÅ“uds, les arcs sont les dÃ©pendances, le type de la relation est l'Ã©tiquette de l'arc.

```python
doc = nlp("Il te refile en stÃ©rÃ©o la chanson des sirÃ¨nes.")
for token in doc:
    print(token, token.dep_, token.head)
```
    > Il expl:subj refile
    te iobj refile
    refile ROOT refile
    en case stÃ©rÃ©o
    stÃ©rÃ©o obl:arg refile
    la det chanson
    chanson obj refile
    des case sirÃ¨nes
    sirÃ¨nes nmod chanson
    . punct refile 

Il existe aussi une reprÃ©sentation graphique de la dÃ©pendance des tokens entre eux.

Dans l'analyse en dÃ©pendance on peut aussi parcourir l'arbre de dÃ©pendance. Les attributs de token suivant peuvent Ãªtre utilisÃ©s pour parcourir l'arbre de dÃ©pendanceÂ :

- `children` les tokens dÃ©pendants du token
- `subtree` tous les descendants du token
- `ancestors` tous les parents du token
- `rights` les enfants Ã  droite du token
- `lefts` les enfants Ã  gauche du token

```python
root = [token for token in doc if token.head == token][0]
subjects = [tok for tok in root.lefts if "subj" in tok.dep_]
subject = subjects[0]
objs = [tok for tok in root.rights if tok.dep_ == "obj"]
obj = objs[0]
print(f"sujetÂ : {subject}, prÃ©dicatÂ : {root}, objetÂ : {obj}")
```
    > sujetÂ : Il, prÃ©dicatÂ : refile, objetÂ : chanson


## Extraction d'information
Ã€ partir de tous ces traitements, il est possible d'effectuer d'autres opÃ©rations.
Spacy a une classe Matcher qui permet de repÃ©rer des tokens ou des sÃ©quences de tokens Ã  l'aide de patrons (pattern).
Ces patrons peuvent porter sur la forme des tokens ou leurs attributs (pos, ent, â€¦).
On peut aussi utiliser des catÃ©gories comme IS_ALPHA ou IS_NUM, voir la doc
(Il existe une dÃ©mo avec interface graphique mais pas pour le franÃ§ais ğŸ™)

```python
from spacy.matcher import Matcher
doc = nlp("Ce modÃ¨le est aussi disponible en taille XLÂ ; je vous le conseille.")
matcher = Matcher(nlp.vocab)

pattern = [{"LOWER": "en"}, {"LOWER": "taille"}, {"IS_ALPHA": True, "IS_UPPER": True}]
#patternÂ : 'en' + 'taille' + lettres en maj

matcher.add("tailles", [pattern])
matches = matcher(doc)
for _, start, end in matches:
    span = doc[start:end]  # The matched span
    print(f"{span.text} ({start}, {end})")
```
    > en taille XL (5, 8)

Ã‡a fonctionne pour les sÃ©quences comme Â« en taille M Â» ou Â« en taille XL Â» mais pas pour Â« vous l'avez en XL ? Â»

C'est un patron de repÃ©rage de sÃ©quences de mots. Il est possible de rechercher aussi des regEx.

Pour les entitÃ©s nommÃ©es, parfois Ã§a fonctionne mal avec le franÃ§ais (prÃ©noms mal Ã©tiquetÃ©, comme misc par exemple), il est donc possible de travailler avec des patrons pour correctement Ã©tiqueter les entitÃ©s nommÃ©es, Ã  partir d'un liste de noms par exemple.

Depuis la v3 de spaCy, il y a un nouvel outil, Dependency Matcher qui permet de faire de l'extraction de patrons syntaxiques. On peut faire porter les requÃªtes sur l'arbre syntaxique et non plus seulement sur la sÃ©quence des tokens.
Permet aussi de tirer des informations sur un texte, Ã  partir de pattern prÃ©cis
Par exemple rÃ©cupÃ©rer tous tokens qui sont les lemmes de Â« acheter Â» ou Â«Â vendre Â», puis d'analyser les dÃ©pendances de ces tokens pour extraite, par exemple, le sujet et l'objet.

Une critique, c'est que spaCy ne prend pas le format sql (pris en charge par txm par exemple).

## Adaptation du modÃ¨le

### Lâ€™exemple des entitÃ©s nommÃ©es
La taille et la nature du corpus d'entraÃ®nement seront dÃ©terminantes. Il est possible d'amender un modÃ¨le existant avec un jeu de donnÃ©es annotÃ©es de taille rÃ©duite.
Exemple sur les entitÃ©s nommÃ©es mais la procÃ¨dure d'entraÃ®nement fonctionne pour d'autres niveaux d'annotations (pos, dÃ©pendance)
Voir la [doc](https://spacy.io/usage/training).

SpaCy propose aussi des outils pour Ã©valuer les performance des modÃ¨les que l'on a gÃ©nÃ©rÃ©, ainsi que des mÃ©canismes d'export et d'import des modÃ¨les et des donnÃ©es.

Il ne faut pas avoir peur de faire son propre modÃ¨le, pour le traitement d'un corpus spÃ©cifique, ce n'est pas forcÃ©ment trÃ¨s long Ã  faire. 

